{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea1a6935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q Table\n",
      "[[0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  1.00000000e+000 0.00000000e+000]\n",
      " [0.00000000e+000 0.00000000e+000 0.00000000e+000 1.26149471e-046\n",
      "  0.00000000e+000 1.00000000e+000]\n",
      " [0.00000000e+000 0.00000000e+000 0.00000000e+000 1.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [0.00000000e+000 2.60273006e-001 3.39726994e-001 0.00000000e+000\n",
      "  4.00000000e-001 0.00000000e+000]\n",
      " [9.78746374e-117 0.00000000e+000 0.00000000e+000 1.50207929e-197\n",
      "  0.00000000e+000 1.00000000e+000]\n",
      " [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  7.22774744e-066 1.00000000e+000]]\n",
      "Optimal Path for starting state 0\n",
      "0 -> 4 -> 5\n",
      "\n",
      "Optimal Path for starting state 1\n",
      "1 -> 5\n",
      "\n",
      "Optimal Path for starting state 2\n",
      "2 -> 3 -> 4 -> 5\n",
      "\n",
      "Optimal Path for starting state 3\n",
      "3 -> 4 -> 5\n",
      "\n",
      "Optimal Path for starting state 4\n",
      "4 -> 5\n",
      "\n",
      "Optimal Path for starting state 5\n",
      "5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    " \n",
    " \n",
    "# defines the reward matrix\n",
    "r = np.array([[-1, -1, -1, -1,  0,  -1],\n",
    "              [-1, -1, -1,  0, -1, 100],\n",
    "              [-1, -1, -1,  0, -1,  -1],\n",
    "              [-1,  0,  0, -1,  0,  -1],\n",
    "              [ 0, -1, -1,  0, -1, 100],\n",
    "              [-1,  0, -1, -1,  0, 100]]).astype(\"float64\")\n",
    "q = np.zeros_like(r)\n",
    " \n",
    " \n",
    "def update_q(state, next_state, action, alpha, gamma):\n",
    "    r_sa = r[state, action]\n",
    "    q_sa = q[state, action]\n",
    "    new_q = q_sa + alpha * (r_sa + gamma * max(q[next_state, :]) - q_sa)\n",
    "    q[state, action] = new_q\n",
    "    # rescale to between 0 and 1\n",
    "    rn = q[state][q[state] > 0]/ np.sum(q[state][q[state] > 0])\n",
    "    q[state][q[state] > 0] = rn\n",
    "    return r[state, action]\n",
    " \n",
    " \n",
    "def show_path():\n",
    "    # show all the paths\n",
    "    for i in range(len(q)):\n",
    "        current_state = i\n",
    "        path = \"%i -> \" % current_state\n",
    "        n_steps = 0\n",
    "        while current_state != 5 and n_steps < 20:\n",
    "            next_state = np.argmax(q[current_state])\n",
    "            current_state = next_state\n",
    "            path += \"%i -> \" % current_state\n",
    "            n_steps = n_steps + 1\n",
    "        # cut off final arrow\n",
    "        path = path[:-4]\n",
    "        print(\"Optimal Path for starting state %i\" % i)\n",
    "        print(path)\n",
    "        print(\"\")\n",
    "        \n",
    "# hyperparameters\n",
    "gamma = 0.4 #vary this between 0 and 1 i.e., (0,1)\n",
    "alpha = 0.9 #vary this between 0 and 1 inclusive, i.e., [0,1]\n",
    "n_episodes = 900 #try different values, e.g., 10, 500, 10000 and so forth\n",
    "epsilon = 0.05 #you can experiment with this as well\n",
    "\n",
    "n_states = 6\n",
    "n_actions = 6\n",
    "\n",
    "random_state = np.random.RandomState(10) #you may try without seed value and other seed values\n",
    "for e in range(int(n_episodes)):\n",
    "    states = list(range(n_states))\n",
    "    random_state.shuffle(states)\n",
    "    current_state = states[0]\n",
    "    goal = False\n",
    "    if e % int(n_episodes / 10.) == 0 and e > 0:\n",
    "        pass\n",
    "    while not goal:\n",
    "        # epsilon greedy\n",
    "        valid_moves = r[current_state] >= 0\n",
    "        if random_state.rand() < epsilon:\n",
    "            actions = np.array(list(range(n_actions)))\n",
    "            actions = actions[valid_moves == True]\n",
    "            if type(actions) is int:\n",
    "                actions = [actions]\n",
    "            random_state.shuffle(actions)\n",
    "            action = actions[0]\n",
    "            next_state = action\n",
    "        else:\n",
    "            if np.sum(q[current_state]) > 0:\n",
    "                action = np.argmax(q[current_state])\n",
    "            else:\n",
    "                # Don't allow invalid moves at the start\n",
    "                # Just take a random move\n",
    "                actions = np.array(list(range(n_actions)))\n",
    "                actions = actions[valid_moves == True]\n",
    "                random_state.shuffle(actions)\n",
    "                action = actions[0]\n",
    "            next_state = action\n",
    "        reward = update_q(current_state, next_state, action,\n",
    "                          alpha=alpha, gamma=gamma)\n",
    "        # Goal due to  rescaling\n",
    "        if reward > 1:\n",
    "            goal = True\n",
    "        current_state = next_state\n",
    "print (\"Q Table\") \n",
    "print(q)\n",
    "show_path()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
